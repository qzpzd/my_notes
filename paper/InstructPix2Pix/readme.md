# [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://github.com/timothybrooks/instruct-pix2pix?spm=a2c6h.12873639.article-detail.5.548e6e7drB8Wqo)

****

[TOC]

- ------



(a) 我们首先使用经过微调的 GPT-3 生成指令和编辑过的字幕。

(b) 然后我们使用 StableDiffusion 结合 Prompt to-Prompt 从成对的字幕生成成对的图像。

(c)我们使用此过程创建包含超过 450,000 个训练示例的数据集 。 

(d) 最后，我们的 InstructPix2Pix 扩散模型根据我们生成的数据进行训练，以根据指令编辑图像。在推理时，我们的模型概括为根据人类编写的指令编辑真实图像

## 示例问题：

### **模型如何根据书面指令获取图像编辑的训练数据**

为了根据书面指令获得用于图像编辑的训练数据，我们的方法结合了两个大型预训练模型的知识：语言模型（GPT-3）和文本到图像模型（稳定扩散）。这种组合生成了一个大型的图像编辑示例数据集，用于训练我们的条件扩散模型 InstructPix2Pix。然后，该模型能够在推理时推广到真实图像和用户编写的指令。（第1页）

### **模型是否可以在不需要对输入或输出图像进行完整描述的情况下编辑图像**

是的，我们的模型可以在不需要对输入或输出图像进行完整描述的情况下编辑图像。我们的模型通过遵循人类编写的指令来编辑图像，而不是通过对输入或输出图像进行完整描述。这使得我们的模型能够快速地编辑图像，只需要几秒钟即可完成。（第1页）

### **模型可以遵循哪些说明示例来编辑图像**

我们的模型可以遵循各种各样的说明示例来编辑图像。在我们的项目页面上，您可以看到一些令人信服的编辑结果，这些结果是基于多种输入图像和书面指令生成的。（第1页）

### **这篇论文主要讲述了什么新的技术观点，分别写出来主要功能有哪些**

本论文提出了一种基于人类指令编辑图像的方法，称为InstructPix2Pix。主要功能包括： 1. 通过结合两个大型预训练模型（语言模型GPT-3和文本到图像模型Stable Diffusion）生成大量的图像编辑示例，以获得此问题的训练数据。 2. 使用生成的数据对条件扩散模型InstructPix2Pix进行训练，并在推理时推广到真实图像和用户编写的指令。 3. 由于它在正向传递中执行编辑并且不需要每个示例进行微调或反演，因此我们的模型可以快速地编辑图像，只需要几秒钟即可完成。（第1页)

